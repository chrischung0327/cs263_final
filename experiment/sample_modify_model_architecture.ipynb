{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f531239ea31b497086a2f2335a9a250b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_f50d1d211e9540859d2b74cd0d906ee6"
          }
        },
        "dd897bb07d074a74bf3067d44c39d043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a02a4d2ec4b41bfafd5dd30dfae997e",
            "placeholder": "​",
            "style": "IPY_MODEL_3afea4a3014b414c8f62cb4490da7e73",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "6ffd3981cbcb48aa9b5226dc23badfce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_6f1ef1125af94d269d90105a1688dedb",
            "placeholder": "​",
            "style": "IPY_MODEL_3a260496698446a085dc531a0989e9e0",
            "value": ""
          }
        },
        "2931144d1fa142b8adfccddcbfcaa737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_56a2ddd35d8d4601a7615580cae2c839",
            "style": "IPY_MODEL_76d90b385f9b4f98bfa7d2d456ca7a35",
            "value": true
          }
        },
        "088f0f23be8440e1ab4edb0b06d40835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e7566a9859174058a52cc2993e5c3fb6",
            "style": "IPY_MODEL_cd772267577a4f27a8e382dc59a71e86",
            "tooltip": ""
          }
        },
        "f81fe517cab04dad945b4158eb91265e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc28b6446ab94bbdaf25996e27c86778",
            "placeholder": "​",
            "style": "IPY_MODEL_e689b7b1452d40dfb60b16473164adf8",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "f50d1d211e9540859d2b74cd0d906ee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "0a02a4d2ec4b41bfafd5dd30dfae997e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3afea4a3014b414c8f62cb4490da7e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f1ef1125af94d269d90105a1688dedb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a260496698446a085dc531a0989e9e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56a2ddd35d8d4601a7615580cae2c839": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76d90b385f9b4f98bfa7d2d456ca7a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7566a9859174058a52cc2993e5c3fb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd772267577a4f27a8e382dc59a71e86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "cc28b6446ab94bbdaf25996e27c86778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e689b7b1452d40dfb60b16473164adf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d1d607dd80a41aba2571a53c449b0cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be389633ad6c466e8fd07c000e8078e5",
            "placeholder": "​",
            "style": "IPY_MODEL_824d224174dc4f6a94551e65bf0fb904",
            "value": "Connecting..."
          }
        },
        "be389633ad6c466e8fd07c000e8078e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "824d224174dc4f6a94551e65bf0fb904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes>0.37.0 trl peft\n",
        "# !pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "oDsBBUJWuAD9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A4hTaED-P0DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "f531239ea31b497086a2f2335a9a250b",
            "dd897bb07d074a74bf3067d44c39d043",
            "6ffd3981cbcb48aa9b5226dc23badfce",
            "2931144d1fa142b8adfccddcbfcaa737",
            "088f0f23be8440e1ab4edb0b06d40835",
            "f81fe517cab04dad945b4158eb91265e",
            "f50d1d211e9540859d2b74cd0d906ee6",
            "0a02a4d2ec4b41bfafd5dd30dfae997e",
            "3afea4a3014b414c8f62cb4490da7e73",
            "6f1ef1125af94d269d90105a1688dedb",
            "3a260496698446a085dc531a0989e9e0",
            "56a2ddd35d8d4601a7615580cae2c839",
            "76d90b385f9b4f98bfa7d2d456ca7a35",
            "e7566a9859174058a52cc2993e5c3fb6",
            "cd772267577a4f27a8e382dc59a71e86",
            "cc28b6446ab94bbdaf25996e27c86778",
            "e689b7b1452d40dfb60b16473164adf8",
            "3d1d607dd80a41aba2571a53c449b0cd",
            "be389633ad6c466e8fd07c000e8078e5",
            "824d224174dc4f6a94551e65bf0fb904"
          ]
        },
        "id": "TuwkukUDuMup",
        "outputId": "f403c325-e9f6-4a02-a070-77ee92981866"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f531239ea31b497086a2f2335a9a250b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional, Tuple, Union"
      ],
      "metadata": {
        "id": "iL58mQ0gzFBz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-yXjsCwPUi_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XhZt0vMwAKP8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "d_kBObwZtyZN"
      },
      "outputs": [],
      "source": [
        "# DO NOT alter this cell\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import LlamaForCausalLM, LlamaConfig\n",
        "from transformers.models.llama.modeling_llama import (\n",
        "    LlamaAttention,\n",
        "    LlamaDecoderLayer,\n",
        "    LlamaModel,\n",
        ")\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# # TODO: Load the model using the appropriate parameters using AutoModelForCausalLM\n",
        "# # Ensure torch_dtype is set to torch.bfloat16\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(device)\n",
        "model.generation_config.temperature = None\n",
        "model.generation_config.top_p = None\n",
        "\n",
        "# TODO: Initialize the tokenizer using AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model # 1B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKv04MtZuREc",
        "outputId": "60ab0926-b689-4cbb-b53d-f8c9019d01b2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2020 Optuna, Hugging Face\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Logging utilities.\"\"\"\n",
        "\n",
        "import functools\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import threading\n",
        "from logging import (\n",
        "    CRITICAL,  # NOQA\n",
        "    DEBUG,  # NOQA\n",
        "    ERROR,  # NOQA\n",
        "    FATAL,  # NOQA\n",
        "    INFO,  # NOQA\n",
        "    NOTSET,  # NOQA\n",
        "    WARN,  # NOQA\n",
        "    WARNING,  # NOQA\n",
        ")\n",
        "from logging import captureWarnings as _captureWarnings\n",
        "from typing import Optional\n",
        "\n",
        "import huggingface_hub.utils as hf_hub_utils\n",
        "from tqdm import auto as tqdm_lib\n",
        "\n",
        "\n",
        "_lock = threading.Lock()\n",
        "_default_handler: Optional[logging.Handler] = None\n",
        "\n",
        "log_levels = {\n",
        "    \"detail\": logging.DEBUG,  # will also print filename and line number\n",
        "    \"debug\": logging.DEBUG,\n",
        "    \"info\": logging.INFO,\n",
        "    \"warning\": logging.WARNING,\n",
        "    \"error\": logging.ERROR,\n",
        "    \"critical\": logging.CRITICAL,\n",
        "}\n",
        "\n",
        "_default_log_level = logging.WARNING\n",
        "\n",
        "_tqdm_active = not hf_hub_utils.are_progress_bars_disabled()\n",
        "\n",
        "\n",
        "def _get_default_logging_level():\n",
        "    \"\"\"\n",
        "    If TRANSFORMERS_VERBOSITY env var is set to one of the valid choices return that as the new default level. If it is\n",
        "    not - fall back to `_default_log_level`\n",
        "    \"\"\"\n",
        "    env_level_str = os.getenv(\"TRANSFORMERS_VERBOSITY\", None)\n",
        "    if env_level_str:\n",
        "        if env_level_str in log_levels:\n",
        "            return log_levels[env_level_str]\n",
        "        else:\n",
        "            logging.getLogger().warning(\n",
        "                f\"Unknown option TRANSFORMERS_VERBOSITY={env_level_str}, \"\n",
        "                f\"has to be one of: { ', '.join(log_levels.keys()) }\"\n",
        "            )\n",
        "    return _default_log_level\n",
        "\n",
        "\n",
        "def _get_library_name() -> str:\n",
        "    return __name__.split(\".\")[0]\n",
        "\n",
        "\n",
        "def _get_library_root_logger() -> logging.Logger:\n",
        "    return logging.getLogger(_get_library_name())\n",
        "\n",
        "\n",
        "def _configure_library_root_logger() -> None:\n",
        "    global _default_handler\n",
        "\n",
        "    with _lock:\n",
        "        if _default_handler:\n",
        "            # This library has already configured the library root logger.\n",
        "            return\n",
        "        _default_handler = logging.StreamHandler()  # Set sys.stderr as stream.\n",
        "        # set defaults based on https://github.com/pyinstaller/pyinstaller/issues/7334#issuecomment-1357447176\n",
        "        if sys.stderr is None:\n",
        "            sys.stderr = open(os.devnull, \"w\")\n",
        "\n",
        "        _default_handler.flush = sys.stderr.flush\n",
        "\n",
        "        # Apply our default configuration to the library root logger.\n",
        "        library_root_logger = _get_library_root_logger()\n",
        "        library_root_logger.addHandler(_default_handler)\n",
        "        library_root_logger.setLevel(_get_default_logging_level())\n",
        "        # if logging level is debug, we add pathname and lineno to formatter for easy debugging\n",
        "        if os.getenv(\"TRANSFORMERS_VERBOSITY\", None) == \"detail\":\n",
        "            formatter = logging.Formatter(\"[%(levelname)s|%(pathname)s:%(lineno)s] %(asctime)s >> %(message)s\")\n",
        "            _default_handler.setFormatter(formatter)\n",
        "\n",
        "        library_root_logger.propagate = False\n",
        "\n",
        "\n",
        "def _reset_library_root_logger() -> None:\n",
        "    global _default_handler\n",
        "\n",
        "    with _lock:\n",
        "        if not _default_handler:\n",
        "            return\n",
        "\n",
        "        library_root_logger = _get_library_root_logger()\n",
        "        library_root_logger.removeHandler(_default_handler)\n",
        "        library_root_logger.setLevel(logging.NOTSET)\n",
        "        _default_handler = None\n",
        "\n",
        "\n",
        "def get_log_levels_dict():\n",
        "    return log_levels\n",
        "\n",
        "\n",
        "def captureWarnings(capture):\n",
        "    \"\"\"\n",
        "    Calls the `captureWarnings` method from the logging library to enable management of the warnings emitted by the\n",
        "    `warnings` library.\n",
        "\n",
        "    Read more about this method here:\n",
        "    https://docs.python.org/3/library/logging.html#integration-with-the-warnings-module\n",
        "\n",
        "    All warnings will be logged through the `py.warnings` logger.\n",
        "\n",
        "    Careful: this method also adds a handler to this logger if it does not already have one, and updates the logging\n",
        "    level of that logger to the library's root logger.\n",
        "    \"\"\"\n",
        "    logger = get_logger(\"py.warnings\")\n",
        "\n",
        "    if not logger.handlers:\n",
        "        logger.addHandler(_default_handler)\n",
        "\n",
        "    logger.setLevel(_get_library_root_logger().level)\n",
        "\n",
        "    _captureWarnings(capture)\n",
        "\n",
        "\n",
        "def get_logger(name: Optional[str] = None) -> logging.Logger:\n",
        "    \"\"\"\n",
        "    Return a logger with the specified name.\n",
        "\n",
        "    This function is not supposed to be directly accessed unless you are writing a custom transformers module.\n",
        "    \"\"\"\n",
        "\n",
        "    if name is None:\n",
        "        name = _get_library_name()\n",
        "\n",
        "    _configure_library_root_logger()\n",
        "    return logging.getLogger(name)\n",
        "\n",
        "\n",
        "def get_verbosity() -> int:\n",
        "    \"\"\"\n",
        "    Return the current level for the 🤗 Transformers's root logger as an int.\n",
        "\n",
        "    Returns:\n",
        "        `int`: The logging level.\n",
        "\n",
        "    <Tip>\n",
        "\n",
        "    🤗 Transformers has following logging levels:\n",
        "\n",
        "    - 50: `transformers.logging.CRITICAL` or `transformers.logging.FATAL`\n",
        "    - 40: `transformers.logging.ERROR`\n",
        "    - 30: `transformers.logging.WARNING` or `transformers.logging.WARN`\n",
        "    - 20: `transformers.logging.INFO`\n",
        "    - 10: `transformers.logging.DEBUG`\n",
        "\n",
        "    </Tip>\"\"\"\n",
        "\n",
        "    _configure_library_root_logger()\n",
        "    return _get_library_root_logger().getEffectiveLevel()\n",
        "\n",
        "\n",
        "def set_verbosity(verbosity: int) -> None:\n",
        "    \"\"\"\n",
        "    Set the verbosity level for the 🤗 Transformers's root logger.\n",
        "\n",
        "    Args:\n",
        "        verbosity (`int`):\n",
        "            Logging level, e.g., one of:\n",
        "\n",
        "            - `transformers.logging.CRITICAL` or `transformers.logging.FATAL`\n",
        "            - `transformers.logging.ERROR`\n",
        "            - `transformers.logging.WARNING` or `transformers.logging.WARN`\n",
        "            - `transformers.logging.INFO`\n",
        "            - `transformers.logging.DEBUG`\n",
        "    \"\"\"\n",
        "\n",
        "    _configure_library_root_logger()\n",
        "    _get_library_root_logger().setLevel(verbosity)\n",
        "\n",
        "\n",
        "def set_verbosity_info():\n",
        "    \"\"\"Set the verbosity to the `INFO` level.\"\"\"\n",
        "    return set_verbosity(INFO)\n",
        "\n",
        "\n",
        "def set_verbosity_warning():\n",
        "    \"\"\"Set the verbosity to the `WARNING` level.\"\"\"\n",
        "    return set_verbosity(WARNING)\n",
        "\n",
        "\n",
        "def set_verbosity_debug():\n",
        "    \"\"\"Set the verbosity to the `DEBUG` level.\"\"\"\n",
        "    return set_verbosity(DEBUG)\n",
        "\n",
        "\n",
        "def set_verbosity_error():\n",
        "    \"\"\"Set the verbosity to the `ERROR` level.\"\"\"\n",
        "    return set_verbosity(ERROR)\n",
        "\n",
        "\n",
        "def disable_default_handler() -> None:\n",
        "    \"\"\"Disable the default handler of the HuggingFace Transformers's root logger.\"\"\"\n",
        "\n",
        "    _configure_library_root_logger()\n",
        "\n",
        "    assert _default_handler is not None\n",
        "    _get_library_root_logger().removeHandler(_default_handler)\n",
        "\n",
        "\n",
        "def enable_default_handler() -> None:\n",
        "    \"\"\"Enable the default handler of the HuggingFace Transformers's root logger.\"\"\"\n",
        "\n",
        "    _configure_library_root_logger()\n",
        "\n",
        "    assert _default_handler is not None\n",
        "    _get_library_root_logger().addHandler(_default_handler)\n",
        "\n",
        "\n",
        "def add_handler(handler: logging.Handler) -> None:\n",
        "    \"\"\"adds a handler to the HuggingFace Transformers's root logger.\"\"\"\n",
        "\n",
        "    _configure_library_root_logger()\n",
        "\n",
        "    assert handler is not None\n",
        "    _get_library_root_logger().addHandler(handler)\n",
        "\n",
        "\n",
        "def remove_handler(handler: logging.Handler) -> None:\n",
        "    \"\"\"removes given handler from the HuggingFace Transformers's root logger.\"\"\"\n",
        "\n",
        "    _configure_library_root_logger()\n",
        "\n",
        "    assert handler is not None and handler not in _get_library_root_logger().handlers\n",
        "    _get_library_root_logger().removeHandler(handler)\n",
        "\n",
        "\n",
        "def disable_propagation() -> None:\n",
        "    \"\"\"\n",
        "    Disable propagation of the library log outputs. Note that log propagation is disabled by default.\n",
        "    \"\"\"\n",
        "\n",
        "    _configure_library_root_logger()\n",
        "    _get_library_root_logger().propagate = False\n",
        "\n",
        "\n",
        "def enable_propagation() -> None:\n",
        "    \"\"\"\n",
        "    Enable propagation of the library log outputs. Please disable the HuggingFace Transformers's default handler to\n",
        "    prevent double logging if the root logger has been configured.\n",
        "    \"\"\"\n",
        "\n",
        "    _configure_library_root_logger()\n",
        "    _get_library_root_logger().propagate = True\n",
        "\n",
        "\n",
        "def enable_explicit_format() -> None:\n",
        "    \"\"\"\n",
        "    Enable explicit formatting for every HuggingFace Transformers's logger. The explicit formatter is as follows:\n",
        "    ```\n",
        "        [LEVELNAME|FILENAME|LINE NUMBER] TIME >> MESSAGE\n",
        "    ```\n",
        "    All handlers currently bound to the root logger are affected by this method.\n",
        "    \"\"\"\n",
        "    handlers = _get_library_root_logger().handlers\n",
        "\n",
        "    for handler in handlers:\n",
        "        formatter = logging.Formatter(\"[%(levelname)s|%(filename)s:%(lineno)s] %(asctime)s >> %(message)s\")\n",
        "        handler.setFormatter(formatter)\n",
        "\n",
        "\n",
        "def reset_format() -> None:\n",
        "    \"\"\"\n",
        "    Resets the formatting for HuggingFace Transformers's loggers.\n",
        "\n",
        "    All handlers currently bound to the root logger are affected by this method.\n",
        "    \"\"\"\n",
        "    handlers = _get_library_root_logger().handlers\n",
        "\n",
        "    for handler in handlers:\n",
        "        handler.setFormatter(None)\n",
        "\n",
        "\n",
        "def warning_advice(self, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    This method is identical to `logger.warning()`, but if env var TRANSFORMERS_NO_ADVISORY_WARNINGS=1 is set, this\n",
        "    warning will not be printed\n",
        "    \"\"\"\n",
        "    no_advisory_warnings = os.getenv(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\", False)\n",
        "    if no_advisory_warnings:\n",
        "        return\n",
        "    self.warning(*args, **kwargs)\n",
        "\n",
        "\n",
        "logging.Logger.warning_advice = warning_advice\n",
        "\n",
        "\n",
        "@functools.lru_cache(None)\n",
        "def warning_once(self, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    This method is identical to `logger.warning()`, but will emit the warning with the same message only once\n",
        "\n",
        "    Note: The cache is for the function arguments, so 2 different callers using the same arguments will hit the cache.\n",
        "    The assumption here is that all warning messages are unique across the code. If they aren't then need to switch to\n",
        "    another type of cache that includes the caller frame information in the hashing function.\n",
        "    \"\"\"\n",
        "    self.warning(*args, **kwargs)\n",
        "\n",
        "\n",
        "logging.Logger.warning_once = warning_once\n",
        "\n",
        "\n",
        "@functools.lru_cache(None)\n",
        "def info_once(self, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    This method is identical to `logger.info()`, but will emit the info with the same message only once\n",
        "\n",
        "    Note: The cache is for the function arguments, so 2 different callers using the same arguments will hit the cache.\n",
        "    The assumption here is that all warning messages are unique across the code. If they aren't then need to switch to\n",
        "    another type of cache that includes the caller frame information in the hashing function.\n",
        "    \"\"\"\n",
        "    self.info(*args, **kwargs)\n",
        "\n",
        "\n",
        "logging.Logger.info_once = info_once\n",
        "\n",
        "\n",
        "\n",
        "class EmptyTqdm:\n",
        "    \"\"\"Dummy tqdm which doesn't do anything.\"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):  # pylint: disable=unused-argument\n",
        "        self._iterator = args[0] if args else None\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self._iterator)\n",
        "\n",
        "    def __getattr__(self, _):\n",
        "        \"\"\"Return empty function.\"\"\"\n",
        "\n",
        "        def empty_fn(*args, **kwargs):  # pylint: disable=unused-argument\n",
        "            return\n",
        "\n",
        "        return empty_fn\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type_, value, traceback):\n",
        "        return\n",
        "\n",
        "\n",
        "class _tqdm_cls:\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        if _tqdm_active:\n",
        "            return tqdm_lib.tqdm(*args, **kwargs)\n",
        "        else:\n",
        "            return EmptyTqdm(*args, **kwargs)\n",
        "\n",
        "    def set_lock(self, *args, **kwargs):\n",
        "        self._lock = None\n",
        "        if _tqdm_active:\n",
        "            return tqdm_lib.tqdm.set_lock(*args, **kwargs)\n",
        "\n",
        "    def get_lock(self):\n",
        "        if _tqdm_active:\n",
        "            return tqdm_lib.tqdm.get_lock()\n",
        "\n",
        "\n",
        "tqdm = _tqdm_cls()\n",
        "\n",
        "\n",
        "def is_progress_bar_enabled() -> bool:\n",
        "    \"\"\"Return a boolean indicating whether tqdm progress bars are enabled.\"\"\"\n",
        "    global _tqdm_active\n",
        "    return bool(_tqdm_active)\n",
        "\n",
        "\n",
        "def enable_progress_bar():\n",
        "    \"\"\"Enable tqdm progress bar.\"\"\"\n",
        "    global _tqdm_active\n",
        "    _tqdm_active = True\n",
        "    hf_hub_utils.enable_progress_bars()\n",
        "\n",
        "\n",
        "def disable_progress_bar():\n",
        "    \"\"\"Disable tqdm progress bar.\"\"\"\n",
        "    global _tqdm_active\n",
        "    _tqdm_active = False\n",
        "    hf_hub_utils.disable_progress_bars()"
      ],
      "metadata": {
        "id": "kUtAR9BExtad"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = get_logger(__name__)\n",
        "\n",
        "def rotate_half(x):\n",
        "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
        "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
        "    \"\"\"\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
        "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
        "\n",
        "    Args:\n",
        "        q (`torch.Tensor`): The query tensor.\n",
        "        k (`torch.Tensor`): The key tensor.\n",
        "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
        "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
        "        position_ids (`torch.Tensor`, *optional*):\n",
        "            Deprecated and unused.\n",
        "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
        "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
        "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
        "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
        "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
        "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
        "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
        "    Returns:\n",
        "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
        "    \"\"\"\n",
        "    cos = cos.unsqueeze(unsqueeze_dim)\n",
        "    sin = sin.unsqueeze(unsqueeze_dim)\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "\n",
        "\n",
        "class CustomLlamaAttention(LlamaAttention):\n",
        "    def __init__(self, config, layer_idx):\n",
        "        super().__init__(config, layer_idx)\n",
        "        # Add your custom initialization or override methods here\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n",
        "        self.num_key_value_heads = config.num_key_value_heads\n",
        "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        position_ids=None,\n",
        "        past_key_value=None,\n",
        "        use_cache=False,\n",
        "        output_attentions=False,\n",
        "        cache_position: Optional[torch.LongTensor] = None,\n",
        "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n",
        "    ):\n",
        "        # Call the original forward method (or implement your custom logic)\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "        query_states = self.q_proj(hidden_states)\n",
        "        key_states = self.k_proj(hidden_states)\n",
        "        value_states = self.v_proj(hidden_states)\n",
        "\n",
        "        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n",
        "        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
        "        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
        "        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        if position_embeddings is None:\n",
        "            logger.warning_once(\n",
        "                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n",
        "                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n",
        "                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n",
        "                \"removed and `position_embeddings` will be mandatory.\"\n",
        "            )\n",
        "            cos, sin = self.rotary_emb(value_states, position_ids)\n",
        "        else:\n",
        "            cos, sin = position_embeddings\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "        if past_key_value is not None:\n",
        "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
        "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
        "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
        "\n",
        "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "        causal_mask = attention_mask\n",
        "        if attention_mask is not None:\n",
        "            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n",
        "\n",
        "        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n",
        "        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n",
        "        if query_states.device.type == \"cuda\" and causal_mask is not None:\n",
        "            query_states = query_states.contiguous()\n",
        "            key_states = key_states.contiguous()\n",
        "            value_states = value_states.contiguous()\n",
        "\n",
        "        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n",
        "        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n",
        "        is_causal = True if causal_mask is None and q_len > 1 else False\n",
        "\n",
        "        attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
        "            query_states,\n",
        "            key_states,\n",
        "            value_states,\n",
        "            attn_mask=causal_mask,\n",
        "            dropout_p=self.attention_dropout if self.training else 0.0,\n",
        "            is_causal=is_causal,\n",
        "        )\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.view(bsz, q_len, -1)\n",
        "\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        return attn_output, None, past_key_value\n",
        "\n",
        "\n",
        "class CustomLlamaDecoderLayer(LlamaDecoderLayer):\n",
        "    def __init__(self, config, layer_idx):\n",
        "        super().__init__(config, layer_idx)\n",
        "        self.self_attn = CustomLlamaAttention(config, layer_idx)\n",
        "\n",
        "class CustomLlamaModel(LlamaModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        # self.layers = torch.nn.ModuleList(\n",
        "        #     [CustomLlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
        "        # )\n",
        "        self.layers = nn.ModuleList(\n",
        "            [CustomLlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
        "        )\n",
        "\n",
        "class CustomLlamaForCausalLM(LlamaForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model = CustomLlamaModel(config)"
      ],
      "metadata": {
        "id": "ohf8-GHOv51s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained configuration and model\n",
        "config = LlamaConfig.from_pretrained(model_id)\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdNP6NqmxcX3",
        "outputId": "a9f415fb-6afc-4246-8245-9942a8547b62"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaConfig {\n",
              "  \"architectures\": [\n",
              "    \"LlamaForCausalLM\"\n",
              "  ],\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 128000,\n",
              "  \"eos_token_id\": [\n",
              "    128001,\n",
              "    128008,\n",
              "    128009\n",
              "  ],\n",
              "  \"head_dim\": 64,\n",
              "  \"hidden_act\": \"silu\",\n",
              "  \"hidden_size\": 2048,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 8192,\n",
              "  \"max_position_embeddings\": 131072,\n",
              "  \"mlp_bias\": false,\n",
              "  \"model_type\": \"llama\",\n",
              "  \"num_attention_heads\": 32,\n",
              "  \"num_hidden_layers\": 16,\n",
              "  \"num_key_value_heads\": 8,\n",
              "  \"pretraining_tp\": 1,\n",
              "  \"rms_norm_eps\": 1e-05,\n",
              "  \"rope_scaling\": {\n",
              "    \"factor\": 32.0,\n",
              "    \"high_freq_factor\": 4.0,\n",
              "    \"low_freq_factor\": 1.0,\n",
              "    \"original_max_position_embeddings\": 8192,\n",
              "    \"rope_type\": \"llama3\"\n",
              "  },\n",
              "  \"rope_theta\": 500000.0,\n",
              "  \"tie_word_embeddings\": true,\n",
              "  \"torch_dtype\": \"bfloat16\",\n",
              "  \"transformers_version\": \"4.46.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 128256\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config.num_hidden_layers = 12\n",
        "config.num_key_value_heads = 4 # Change as you want. Default: 8"
      ],
      "metadata": {
        "id": "Ks-Pby3k0ZYf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijeA6S5K5V0I",
        "outputId": "98d44653-21d8-4c7f-cccf-b28c0ffe79f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaConfig {\n",
              "  \"_attn_implementation_autoset\": true,\n",
              "  \"architectures\": [\n",
              "    \"LlamaForCausalLM\"\n",
              "  ],\n",
              "  \"attention_bias\": false,\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"bos_token_id\": 128000,\n",
              "  \"eos_token_id\": [\n",
              "    128001,\n",
              "    128008,\n",
              "    128009\n",
              "  ],\n",
              "  \"head_dim\": 64,\n",
              "  \"hidden_act\": \"silu\",\n",
              "  \"hidden_size\": 2048,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 8192,\n",
              "  \"max_position_embeddings\": 131072,\n",
              "  \"mlp_bias\": false,\n",
              "  \"model_type\": \"llama\",\n",
              "  \"num_attention_heads\": 32,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"num_key_value_heads\": 4,\n",
              "  \"pretraining_tp\": 1,\n",
              "  \"rms_norm_eps\": 1e-05,\n",
              "  \"rope_scaling\": {\n",
              "    \"factor\": 32.0,\n",
              "    \"high_freq_factor\": 4.0,\n",
              "    \"low_freq_factor\": 1.0,\n",
              "    \"original_max_position_embeddings\": 8192,\n",
              "    \"rope_type\": \"llama3\"\n",
              "  },\n",
              "  \"rope_theta\": 500000.0,\n",
              "  \"tie_word_embeddings\": true,\n",
              "  \"torch_dtype\": \"bfloat16\",\n",
              "  \"transformers_version\": \"4.46.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 128256\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of your custom model\n",
        "custom_model = CustomLlamaForCausalLM(config).to(device)"
      ],
      "metadata": {
        "id": "enzktH6Lz381"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict = model.state_dict()\n",
        "custom_model_dict = custom_model.state_dict()\n",
        "\n",
        "# Filter out weights for the custom attention layers\n",
        "filtered_pretrained_dict = {\n",
        "    k: v for k, v in model_dict.items() if k in custom_model_dict and 'self_attn' not in k\n",
        "}\n",
        "\n",
        "# Update the existing model's state dict\n",
        "custom_model_dict.update(filtered_pretrained_dict)\n",
        "custom_model.load_state_dict(custom_model_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAEKSrc9OI18",
        "outputId": "0db0507d-45b9-40f0-8868-8c720baf4e81"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW8zWFoIQjG0",
        "outputId": "265bf706-9fe9-4acc-fa4d-b9b2a71a14ff"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomLlamaForCausalLM(\n",
              "  (model): CustomLlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-11): 12 x CustomLlamaDecoderLayer(\n",
              "        (self_attn): CustomLlamaAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define the messages for the chatbot interaction (List[Dict])\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a chatbot who responds very shortly.\"},\n",
        "    {\"role\": \"user\", \"content\": \"When was UCLA founded?\"},\n",
        "]\n",
        "\n",
        "def run_model(model, tokenizer, messages, max_new_tokens=5, verbose=False):\n",
        "    # TODO: Prepare the input text using the tokenizer's apply_chat_template (Do not tokenize the text yet)\n",
        "    input_text = \"<|begin_of_text|>\"\n",
        "    for message in messages:\n",
        "        role = message[\"role\"]\n",
        "        content = message[\"content\"]\n",
        "        input_text += f'<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}<|eot_id|>'\n",
        "\n",
        "    input_text += '<|start_header_id|>assistance<|end_header_id|>'\n",
        "    # input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "    if verbose: print(\"\\n###input_text:###\\n\", input_text)\n",
        "    # TODO: Tokenize the input text and transfer it to the appropriate device\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    if verbose: print(\"\\n###input_ids:###\\n\", inputs.input_ids)\n",
        "    # TODO: Generate a response using the model. Ensure do_sample is False.\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id  # Prevents padding errors\n",
        "    )\n",
        "\n",
        "    # TODO: Decode the output and return the response without special tokens\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if verbose: print(\"\\n###response:###\\n\", response)\n",
        "    assistant_response = response.split('assistance')[-1].replace(\"\\n\", \" \").strip()\n",
        "    return assistant_response\n",
        "\n",
        "assistant_response = run_model(model=custom_model, tokenizer=tokenizer, messages=messages, max_new_tokens=10, verbose=False)\n",
        "print(f\"\\n###Assistant response:###\\n{assistant_response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1JmLp_lNK1L",
        "outputId": "23d138cd-6da9-4d74-d064-606277231ba7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "###Assistant response:###\n",
            "ffffffffffffffffffff\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1XU6XLK0xCqX"
      }
    }
  ]
}